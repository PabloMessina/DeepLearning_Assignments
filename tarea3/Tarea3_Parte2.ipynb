{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 4\n",
    "#### completar los TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    FREQUENCY_THRESHOLD = 2\n",
    "    VOCAB_SIZE = 50000\n",
    "    PAD_TOKEN = \"<pad>\"\n",
    "    START_TOKEN = \"<s>\"\n",
    "    END_TOKEN = \"</s>\"\n",
    "    UNKNOWN_TOKEN = \"<unk>\"\n",
    "    \n",
    "    def __init__(self, path, src, dst, src_token2id=None, src_id2token=None, dst_token2id=None, dst_id2token=None):\n",
    "        assert src in (\"es\", \"en\")\n",
    "        assert dst != src and dst in (\"es\", \"en\")\n",
    "        \n",
    "        self.src_sentences = self._load_sentences(path, src)\n",
    "        self.dst_sentences = self._load_sentences(path, dst)\n",
    "        assert len(self.src_sentences) == len(self.dst_sentences)\n",
    "        \n",
    "        if src_token2id is None and src_id2token is None and dst_token2id is None and dst_id2token is None:\n",
    "            self.src_token2id, self.src_id2token = self._build_token2id(self.src_sentences)\n",
    "            self.dst_token2id, self.dst_id2token = self._build_token2id(self.dst_sentences)\n",
    "        else:\n",
    "            self.src_token2id, self.src_id2token = src_token2id, src_id2token\n",
    "            self.dst_token2id, self.dst_id2token = dst_token2id, dst_id2token\n",
    "    \n",
    "    def _load_sentences(self, path, lang):\n",
    "        path = path + \".{}\".format(lang)\n",
    "        with open(path, \"r\") as corpus_file:\n",
    "            sentences = [line.strip().split(\" \") for line in corpus_file]\n",
    "        return sentences\n",
    "    \n",
    "    def _build_token2id(self, sentences):\n",
    "        \"\"\"\n",
    "        [TODO] Debe implementar este método.\n",
    "        Recibe como parámetros una lista con listas de tokens de una oración.\n",
    "        Se debe retornar:\n",
    "            - `token2id`: un diccionario con el vocabulario correspondiente a las VOCAB_SIZE palabras\n",
    "                más frecuentes que cumplan con tener una frecuencia mayor o igual a FREQUENCY_THRESHOLD.\n",
    "                Cada llave es un token asociado a un identificador único entero.\n",
    "            - `id2token`: un diccionario con el mismo vocabulario de `token2id`, pero esta vez cada llave\n",
    "                el es identificador único asociado al respectivo token.\n",
    "                \n",
    "        IMPORTANTE: debe agregar al principio del vocabulario los tokens especiales self.PAD_TOKEN,\n",
    "            self.START_TOKEN, self.END_TOKEN y self.UNKNOWN_TOKEN, que quedarán con los identificadores\n",
    "            0, 1, 2 y 3 respectivamente. Por lo tanto, el vocabulario puede quedar con un tamaño máximo\n",
    "            de VOCAB_SIZE + 4 elementos.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        token2count = dict()\n",
    "        for sentence in sentences:\n",
    "            for token in sentence:\n",
    "                token2count[token] = token2count.get(token, 0) + 1\n",
    "        pairs = list(token2count.items())\n",
    "        pairs.sort(key=lambda p:p[1], reverse=True)\n",
    "        id2token = [self.PAD_TOKEN, self.START_TOKEN, self.END_TOKEN, self.UNKNOWN_TOKEN]\n",
    "        id2token.extend(pairs[i][0] for i in range(self.VOCAB_SIZE))\n",
    "        token2id = { token:_id for _id, token in enumerate(id2token) }\n",
    "        \n",
    "        return token2id, id2token\n",
    "\n",
    "    def _tokens2ids(self, token2id, tokens, is_target=False):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in token2id:\n",
    "                ids.append(token2id[token])\n",
    "            else:\n",
    "                ids.append(token2id[self.UNKNOWN_TOKEN])\n",
    "        if is_target:\n",
    "            ids.insert(0, token2id[self.START_TOKEN])\n",
    "            ids.append(token2id[self.END_TOKEN])\n",
    "        return ids\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens, dst_tokens = self.src_sentences[idx], self.dst_sentences[idx]\n",
    "        src_ids, dst_ids = self._tokens2ids(self.src_token2id, src_tokens), self._tokens2ids(self.dst_token2id, dst_tokens, is_target=True)\n",
    "        return {\n",
    "            \"src\": src_ids,\n",
    "            \"dst\": dst_ids\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def ids2tokens(self, id2token, ids):\n",
    "        tokens = [id2token[id] for id in ids]\n",
    "        return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./en_es_data/train\"\n",
    "TEST_PATH = \"./en_es_data/test\"\n",
    "DEV_PATH = \"./en_es_data/dev\"\n",
    "\n",
    "train_dataset = CorpusDataset(\n",
    "    path=TRAIN_PATH,\n",
    "    src=\"en\",\n",
    "    dst=\"es\")\n",
    "dev_dataset = CorpusDataset(\n",
    "    path=DEV_PATH,\n",
    "    src=\"en\",\n",
    "    dst=\"es\",\n",
    "    src_token2id=train_dataset.src_token2id,\n",
    "    src_id2token=train_dataset.src_id2token,\n",
    "    dst_token2id=train_dataset.dst_token2id,\n",
    "    dst_id2token=train_dataset.dst_id2token)\n",
    "test_dataset = CorpusDataset(\n",
    "    path=TEST_PATH,\n",
    "    src=\"en\",\n",
    "    dst=\"es\",\n",
    "    src_token2id=train_dataset.src_token2id,\n",
    "    src_id2token=train_dataset.src_id2token,\n",
    "    dst_token2id=train_dataset.dst_token2id,\n",
    "    dst_id2token=train_dataset.dst_id2token)\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_sentences(batch):\n",
    "    \"\"\"\n",
    "    Esta función permite construir lotes de pares de oraciones en lenguaje de origen y objetivo.\n",
    "    Al utilizar secuencias, en este caso oraciones, de largo variable en redes recurrentes es necesario\n",
    "    empaquetarlas para hacer cálculos eficientes en PyTorch. En esta tarea particular, solo se\n",
    "    necesita empaquetar las oraciones de entrada. Para empaquetar secuencias, es requisito que estén\n",
    "    ordenadas de acuerdo a su largo en forma decreciente.\n",
    "    \n",
    "    Si se desea profundizar:\n",
    "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
    "        https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "    \"\"\"\n",
    "    src = []\n",
    "    dst = []\n",
    "    for item in batch:\n",
    "        src.append(torch.tensor(item[\"src\"]))\n",
    "        dst.append(torch.tensor(item[\"dst\"]))\n",
    "    src_dst_zipped = list(zip(src, dst))\n",
    "    src_dst_zipped.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    batch_dict = {\n",
    "        \"src\": [],\n",
    "        \"dst\": [],\n",
    "        \"src_lengths\": [],\n",
    "        \"dst_lengths\": []\n",
    "    }\n",
    "    \n",
    "    for pair in src_dst_zipped:\n",
    "        batch_dict[\"src\"].append(pair[0])\n",
    "        batch_dict[\"src_lengths\"].append(len(pair[0]))\n",
    "        batch_dict[\"dst\"].append(pair[1])\n",
    "        batch_dict[\"dst_lengths\"].append(len(pair[1]))\n",
    "        \n",
    "    \n",
    "    for elem in (\"src\", \"dst\"):\n",
    "        lengths_name = \"%s_lengths\" % elem\n",
    "        batch_dict[lengths_name] = torch.tensor(batch_dict[lengths_name])\n",
    "        batch_dict[elem] = nn.utils.rnn.pad_sequence(\n",
    "            sequences=batch_dict[elem],\n",
    "            batch_first=True,\n",
    "            padding_value=0)\n",
    "        \n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_sentences)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_sentences)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_sentences)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict(\n",
    "    train=train_dataloader,\n",
    "    dev=dev_dataloader,\n",
    "    test=test_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _EncoderModule(nn.Module):\n",
    "    def __init__(self, embeddings_table, embeddings_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_table = embeddings_table\n",
    "        self.embeddings_size = embeddings_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \"\"\"\n",
    "        [TODO] Inicializar los siguientes atributos:\n",
    "            self.lstm: LSTM bidireccional con bias\n",
    "        \n",
    "        Documentación relevante:\n",
    "            https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embeddings_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )        \n",
    "        \n",
    "        \n",
    "    def forward(self, src_sentences, src_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - src_sentences: un tensor con forma [batch_sz, max_src_sentence_length].\n",
    "            - src_lengths: un tensor con forma [batch_sz].\n",
    "        Returns:\n",
    "            - all_hidden_states: un tensor con forma [batch_sz, max_src_sentence_length, 2 * hidden_size]\n",
    "                Son los h^{enc}_i del enunciado.\n",
    "            - hidden_states: un tensor con forma [2, batch_sz, hidden_size].\n",
    "                Son los h^{enc} finales de la pasada hacia delante y hacia atrás de la LSTM bidireccional.\n",
    "            - cell_states: un tensor con forma [2, batch_sz, hidden_size].\n",
    "                Son los c^{enc} finales de la pasada hacia delante y hacia atrás de la LSTM bidireccional.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        [TODO] Implemente el `forward` del codificador.\n",
    "            1. Debe calcular los embeddings de las oraciones.\n",
    "            2. Debe empacar las secuencias de embeddings.\n",
    "            3. Debe pasar la secuencia empacada por la LSTM bidireccional.\n",
    "            4. Debe rellenar los estados ocultos empaquetados retornados por la LSTM.\n",
    "        \n",
    "        Documentación relevante:\n",
    "            https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n",
    "            https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n",
    "        \"\"\"\n",
    "        embedded_sentences = self.embeddings_table(src_sentences)\n",
    "        packed_sentences = pack_padded_sequence(embedded_sentences,\n",
    "                                                src_lengths,\n",
    "                                                batch_first=True)\n",
    "        \n",
    "        batch_size, max_src_sentence_length = src_sentences.shape\n",
    "        \n",
    "        assert max_src_sentence_length == src_lengths.max()\n",
    "        \n",
    "        h0 = self.init_hidden_state(batch_size)\n",
    "        c0 = self.init_cell_state(batch_size)\n",
    "        output, (hidden_states, cell_states) = self.lstm(packed_sentences, (h0, c0))\n",
    "        all_hidden_states, _ = pad_packed_sequence(output,\n",
    "                                                   batch_first=True,\n",
    "                                                   padding_value=0,\n",
    "                                                   total_length=max_src_sentence_length)\n",
    "\n",
    "        assert all_hidden_states.shape == (batch_size, max_src_sentence_length, 2 * self.hidden_size)\n",
    "        assert hidden_states.shape == (2, batch_size, self.hidden_size)\n",
    "        assert cell_states.shape == (2, batch_size, self.hidden_size)\n",
    "        \n",
    "        return all_hidden_states, hidden_states, cell_states\n",
    "    \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size).to(DEVICE)\n",
    "    \n",
    "    def init_cell_state(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size).to(DEVICE)\n",
    "\n",
    "class _DecoderModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embeddings_table,\n",
    "            embeddings_size,\n",
    "            hidden_size,\n",
    "            start_idx,\n",
    "            dropout_prob,\n",
    "            dst_vocab_size\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_table = embeddings_table\n",
    "        self.embeddings_size = embeddings_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.start_idx = torch.tensor(start_idx).to(DEVICE)\n",
    "        self.dst_vocab_size = dst_vocab_size\n",
    "        \n",
    "        \"\"\"\n",
    "        [TODO] Inicializar los siguientes atributos:\n",
    "            self.W_h: capa lineal sin bias.\n",
    "            self.W_c: capa lineal sin bias.\n",
    "            self.lstm_cell: celda LSTM con bias.\n",
    "            self.W_attn: capa lineal sin bias.\n",
    "            self.W_u: capa lineal sin bias.\n",
    "            self.dropout: capa de dropout.\n",
    "            self.W_vocab: capa lineal sin bias.\n",
    "        \n",
    "        Documentación relevante:\n",
    "            https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "            https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n",
    "            https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "            https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "        \"\"\"        \n",
    "        self.W_h = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.W_c = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=embeddings_size+hidden_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     bias=True)\n",
    "        self.W_attn = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.W_u = nn.Linear(3 * hidden_size, hidden_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.W_vocab = nn.Linear(hidden_size, dst_vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            all_enc_hidden_states,\n",
    "            final_enc_hidden_states,\n",
    "            final_enc_cell_states,\n",
    "            dst_sentences,\n",
    "            dst_lengths,            \n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - all_enc_hidden_states: un tensor con forma [batch_sz, max_sent_length, 2 * hidden_size].\n",
    "            - final_enc_hidden_states: un tensor con forma [2, batch_sz, hidden_size].\n",
    "            - final_enc_cell_states: un tensor con forma [2, batch_sz, hidden_size].\n",
    "            - max_sentence_length: un int con el largo máximo de las oraciones en el lenguaje objetivo.\n",
    "        Returns:\n",
    "            - output: un tensor con forma [batch_sz, max_sent_length, dst_vocab_size]. Corresponde al tensor\n",
    "                DENTRO DE LA FUNCIÓN SOFTMAX en la ecuación donde se define P_t.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        [TODO] Implemente el `forward` del decodificador. Recuerde:\n",
    "            - debe inicializar los vectores que se indican en el enunciado.\n",
    "            - retorne el tensor DENTRO DE LA FUNCIÓN SOFTMAX de la ecuación donde se define P_t, esto\n",
    "                es importante debido a que la función de pérdida (i.e. CrossEntropyLoss) aplica la\n",
    "                función softmax para tener mejor estabilidad numérica.\n",
    "                \n",
    "        Documentación relevante:\n",
    "            https://pytorch.org/docs/stable/torch.html#torch.zeros\n",
    "            https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
    "            https://pytorch.org/docs/stable/torch.html#torch.cat\n",
    "            https://pytorch.org/docs/stable/torch.html#torch.sum\n",
    "            https://pytorch.org/docs/stable/torch.html#torch.argmax\n",
    "            https://pytorch.org/docs/stable/torch.html#torch.stack\n",
    "        \"\"\"\n",
    "        max_sentence_length = dst_lengths.max()\n",
    "        batch_size = all_enc_hidden_states.shape[0]\n",
    "        y = self.embeddings_table(self.start_idx).expand(batch_size, -1)\n",
    "        o = torch.zeros(batch_size, self.hidden_size).to(DEVICE)\n",
    "        h = self.W_h(torch.cat((final_enc_hidden_states[0],\n",
    "                                final_enc_hidden_states[1]),1))  # h0_dec\n",
    "        c = self.W_c(torch.cat((final_enc_cell_states[0],\n",
    "                                final_enc_cell_states[1]),1))  # c0_dec\n",
    "        sentence_embeddings = self.embeddings_table(dst_sentences.permute(1,0))\n",
    "        assert sentence_embeddings.shape == (max_sentence_length, batch_size, self.embeddings_size)\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        for t in range(max_sentence_length):\n",
    "            y_bar = torch.cat((y,o),1)\n",
    "            assert y_bar.shape == (batch_size, self.embeddings_size + self.hidden_size)\n",
    "            assert h.shape == (batch_size, self.hidden_size)\n",
    "            assert c.shape == (batch_size, self.hidden_size)\n",
    "            h, c = self.lstm_cell(y_bar, (h, c))\n",
    "            e = (self.W_attn(all_enc_hidden_states) * h.unsqueeze(1)).sum(-1)\n",
    "            att = torch.softmax(e,-1)\n",
    "            a = (all_enc_hidden_states * att.unsqueeze(2)).sum(1)\n",
    "            assert a.shape == (batch_size, 2 * self.hidden_size)\n",
    "            u = torch.cat((a,h),1)\n",
    "            assert u.shape == (batch_size, 3 * self.hidden_size)\n",
    "            v = self.W_u(u)\n",
    "            o = self.dropout(torch.tanh(v))\n",
    "            assert o.shape == (batch_size, self.hidden_size)\n",
    "            output.append(self.W_vocab(o))\n",
    "            y = sentence_embeddings[t]\n",
    "        \n",
    "        output = torch.stack(output, 1)\n",
    "        assert output.shape == (batch_size, max_sentence_length, self.dst_vocab_size)\n",
    "        return output\n",
    "    \n",
    "    def eval_forward(\n",
    "            self,\n",
    "            all_enc_hidden_states,\n",
    "            final_enc_hidden_states,\n",
    "            final_enc_cell_states,\n",
    "            dst_lengths,\n",
    "        ):\n",
    "        \n",
    "        max_sentence_length = dst_lengths.max()\n",
    "        batch_size = all_enc_hidden_states.shape[0]\n",
    "        y = self.embeddings_table(self.start_idx).expand(batch_size, -1)\n",
    "        o = torch.zeros(batch_size, self.hidden_size).to(DEVICE)\n",
    "        h = self.W_h(torch.cat((final_enc_hidden_states[0],\n",
    "                                final_enc_hidden_states[1]),1))  # h0_dec\n",
    "        c = self.W_c(torch.cat((final_enc_cell_states[0],\n",
    "                                final_enc_cell_states[1]),1))  # c0_dec        \n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        for t in range(max_sentence_length):\n",
    "            y_bar = torch.cat((y,o),1)\n",
    "            assert y_bar.shape == (batch_size, self.embeddings_size + self.hidden_size)\n",
    "            assert h.shape == (batch_size, self.hidden_size)\n",
    "            assert c.shape == (batch_size, self.hidden_size)\n",
    "            h, c = self.lstm_cell(y_bar, (h, c))\n",
    "            e = (self.W_attn(all_enc_hidden_states) * h.unsqueeze(1)).sum(-1)\n",
    "            att = torch.softmax(e,-1)\n",
    "            a = (all_enc_hidden_states * att.unsqueeze(2)).sum(1)\n",
    "            assert a.shape == (batch_size, 2 * self.hidden_size)\n",
    "            u = torch.cat((a,h),1)\n",
    "            assert u.shape == (batch_size, 3 * self.hidden_size)\n",
    "            v = self.W_u(u)\n",
    "            o = self.dropout(torch.tanh(v))\n",
    "            assert o.shape == (batch_size, self.hidden_size)\n",
    "            output.append(self.W_vocab(o))\n",
    "            y = self.embeddings_table(torch.argmax(output[t], 1))\n",
    "            assert y.shape == (batch_size, self.embeddings_size)\n",
    "        \n",
    "        output = torch.stack(output, 1)\n",
    "        assert output.shape == (batch_size, max_sentence_length, self.dst_vocab_size)\n",
    "        return output\n",
    "    \n",
    "class NeuralMachineTranslator(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            src_vocab_size,\n",
    "            dst_vocab_size,\n",
    "            start_idx,\n",
    "            embeddings_size,\n",
    "            hidden_size,\n",
    "            dropout_prob\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        [TODO] Inicializar los siguientes atributos:\n",
    "            - self.src_embeddings_table: matriz de embeddings.\n",
    "            - self.dst_embeddings_table: matriz de embeddings.\n",
    "            \n",
    "        Documentación relevante:\n",
    "            - https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "            \n",
    "        OJO: recuerde asignar al parámetro `padding_idx` el identificador del token de relleno.\n",
    "        \"\"\"\n",
    "        self.src_embeddings_table = nn.Embedding(\n",
    "            num_embeddings=src_vocab_size,\n",
    "            embedding_dim=embeddings_size,\n",
    "            padding_idx=0,            \n",
    "        ).to(DEVICE)\n",
    "        self.dst_embeddings_table = nn.Embedding(\n",
    "            num_embeddings=dst_vocab_size,\n",
    "            embedding_dim=embeddings_size,\n",
    "            padding_idx=0,\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        self.encoder_module = _EncoderModule(\n",
    "            embeddings_table=self.src_embeddings_table,\n",
    "            embeddings_size=embeddings_size,\n",
    "            hidden_size=hidden_size)\n",
    "        self.decoder_module = _DecoderModule(\n",
    "            embeddings_table=self.dst_embeddings_table,\n",
    "            embeddings_size=embeddings_size,\n",
    "            hidden_size=hidden_size,\n",
    "            start_idx=start_idx,\n",
    "            dropout_prob=dropout_prob,\n",
    "            dst_vocab_size=dst_vocab_size)\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            src_sentences,\n",
    "            src_lengths,\n",
    "            dst_sentences,\n",
    "            dst_lengths\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - src_sentences: un tensor con forma [batch_sz, max_src_sentence_length].\n",
    "                Son los identificadores de los tokens de las oraciones en el lenguaje de origen.\n",
    "            - src_lengths: un tensor con forma [batch_sz].\n",
    "                Son los largos de las oraciones en el lenguaje de origen.\n",
    "            - dst_lengths: un tensor con forma [batch_sz].\n",
    "                Son los largos de las oraciones en el lenguaje objetivo.\n",
    "        Returns:\n",
    "            - output: un tensor con forma [batch_sz, max_dst_sentence_length, dst_vocab_sz].\n",
    "                Son las distribuciones de probabilidades sobre el vocabulario del lenguaje objetivo.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        [TODO] Implemente el `forward` del NeuralMachineTranslator de acuerdo al enunciado.\n",
    "        \"\"\"\n",
    "        \n",
    "        all_enc_hidden_states,\\\n",
    "        final_enc_hidden_states,\\\n",
    "        final_enc_cell_states = self.encoder_module(src_sentences, src_lengths)\n",
    "        \n",
    "        output = self.decoder_module(\n",
    "            all_enc_hidden_states,\n",
    "            final_enc_hidden_states,\n",
    "            final_enc_cell_states,\n",
    "            dst_sentences,\n",
    "            dst_lengths)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def eval_forward(\n",
    "            self,\n",
    "            src_sentences,\n",
    "            src_lengths,\n",
    "            dst_lengths,\n",
    "        ):\n",
    "        \n",
    "        all_enc_hidden_states,\\\n",
    "        final_enc_hidden_states,\\\n",
    "        final_enc_cell_states = self.encoder_module(src_sentences, src_lengths)\n",
    "        \n",
    "        output = self.decoder_module.eval_forward(\n",
    "            all_enc_hidden_states,\n",
    "            final_enc_hidden_states,\n",
    "            final_enc_cell_states,\n",
    "            dst_lengths)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Device: \", DEVICE)\n",
    "\n",
    "EMBEDDINGS_SIZE = 256\n",
    "HIDDEN_SIZE = 256\n",
    "GRADIENT_CLIPPING = 5\n",
    "DECAY_PATIENCE = 5\n",
    "LR_DECAY = 0.5\n",
    "LR = 0.001\n",
    "DROPOUT_PROB = 0.3\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt = NeuralMachineTranslator(\n",
    "    src_vocab_size=len(train_dataset.src_token2id),\n",
    "    dst_vocab_size=len(train_dataset.dst_token2id),\n",
    "    start_idx=train_dataset.src_token2id[train_dataset.START_TOKEN],\n",
    "    embeddings_size=EMBEDDINGS_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout_prob=DROPOUT_PROB)\n",
    "nmt.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nmt.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mean_bleu_score(gt_sentences, pred_sentences, smoothing_function):\n",
    "    assert gt_sentences.shape == pred_sentences.shape\n",
    "    bleu = 0\n",
    "    n = gt_sentences.shape[0]\n",
    "    for i in range(n):\n",
    "        reference = [t for t in gt_sentences[i] if t >= 4]\n",
    "        hypothesis = [t for t in pred_sentences[i] if t >= 4]\n",
    "        bleu += sentence_bleu((reference,), hypothesis, smoothing_function=smoothing_function)\n",
    "    return bleu/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[TODO] Implemente el training/evaluation loop del NeuralMachineTranslator.\n",
    "\n",
    "Disminuya la tasa de aprendizaje (multiplicándola por LR_DECAY) cuando la pérdida en validación\n",
    "no mejore en DECAY_PATIENCE épocas.\n",
    "\n",
    "Realice un clipping de gradientes si es que la norma de éstos superan GRADIENT_CLIPPING.\n",
    "\n",
    "OJO: los objetos que heredan de `nn.Module` tienen un modo de entrenamiento\n",
    "y evaluación que se configura mediante el método `model.train(mode)`. Si va a evaluar\n",
    "el modelo recuerde detener el modo entrenamiento a fin de apagar la capa de dropout.\n",
    "\"\"\"\n",
    "\n",
    "def train_model(nmt, optimizer, criterion, scheduler, dataloaders, epochs, checkpoint_path=None):\n",
    "    \n",
    "    since = time.time()    \n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    \n",
    "    if checkpoint_path is None:\n",
    "        print('training from scratch ....')\n",
    "        epoch_one = 1\n",
    "        best_wts = copy.deepcopy(nmt.state_dict())\n",
    "        best_dev_bleu = -1e9\n",
    "        best_train_bleu = -1e9\n",
    "        history = dict()        \n",
    "        for met in ('loss', 'bleu'):\n",
    "            for phase in ('train', 'dev'):\n",
    "                history['%s_%s' % (phase,met)] = []\n",
    "    else:\n",
    "        print('resuming training from checkpoint = %s ....' % checkpoint_path)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        best_wts = checkpoint['best_wts']\n",
    "        best_dev_bleu = checkpoint['best_dev_bleu']\n",
    "        best_train_bleu = checkpoint['best_train_bleu']\n",
    "        history = checkpoint['history']\n",
    "        epoch_one = len(history['train_loss'])\n",
    "        nmt.load_state_dict(best_wts)\n",
    "        print('\\tbest_dev_bleu = ', best_dev_bleu)\n",
    "\n",
    "    for epoch in range(epoch_one, epochs+1):\n",
    "        print('\\n====== [Epoch {}/{}]'.format(epoch, epochs))\n",
    "        \n",
    "        for phase in ('train', 'dev'):\n",
    "            \n",
    "            print('--------------------- %s ---------------------' % phase)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                nmt.train()\n",
    "            else:\n",
    "                nmt.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_bleu = 0\n",
    "            running_count = 0\n",
    "            epoch_since = time.time()\n",
    "            dataloader = dataloaders[phase]\n",
    "                \n",
    "            with torch.set_grad_enabled(phase == 'train'):                \n",
    "                \n",
    "                for batch_idx, batch_dict in enumerate(dataloader):\n",
    "                    \n",
    "                    src_sentences = batch_dict['src'].to(DEVICE)\n",
    "                    src_lengths = batch_dict['src_lengths'].to(DEVICE)\n",
    "                    dst_sentences = batch_dict['dst'].to(DEVICE)\n",
    "                    dst_lengths = batch_dict['dst_lengths'].to(DEVICE)\n",
    "                    batch_size = src_sentences.size(0)\n",
    "                    \n",
    "                    optimizer.zero_grad()                    \n",
    "                    \n",
    "                    # forward pass\n",
    "                    if phase == 'train':\n",
    "                        output = nmt(\n",
    "                            src_sentences,\n",
    "                            src_lengths,\n",
    "                            dst_sentences,\n",
    "                            dst_lengths)\n",
    "                    else:\n",
    "                        output = nmt.eval_forward(\n",
    "                            src_sentences,\n",
    "                            src_lengths,\n",
    "                            dst_lengths)\n",
    "                    \n",
    "                    loss = criterion(output.view(-1,output.shape[-1]),\n",
    "                                     dst_sentences.view(-1))\n",
    "                    bleu_score = batch_mean_bleu_score(dst_sentences.cpu().numpy(),\n",
    "                                                       output.argmax(-1).cpu().numpy(),\n",
    "                                                       smoothing_function)\n",
    "                    \n",
    "                    # backward pass + optimization only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        clip_grad_norm_(nmt.parameters(), GRADIENT_CLIPPING)\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * batch_size\n",
    "                    running_bleu += bleu_score * batch_size\n",
    "                    running_count += batch_size\n",
    "                    \n",
    "                    if (batch_idx % 50 == 0 or batch_idx + 1 == len(dataloader)):\n",
    "                        elapsed_time = time.time() - epoch_since\n",
    "                        print(\"Batch: %d/%d, running_loss=%.5f, running_bleu=%.5f, elapsed_time=%.0fm %.0fs\" % (\n",
    "                            batch_idx+1, len(dataloader),\n",
    "                            running_loss/running_count,\n",
    "                            running_bleu/running_count,\n",
    "                            elapsed_time // 60, elapsed_time % 60,\n",
    "                        ), end=\"\\r\", flush=True)\n",
    "            print()\n",
    "                    \n",
    "            epoch_loss = running_loss / running_count\n",
    "            epoch_bleu = running_bleu / running_count\n",
    "            history['%s_loss' % phase].append(epoch_loss)\n",
    "            history['%s_bleu' % phase].append(epoch_bleu)\n",
    "            \n",
    "            if phase == 'dev':\n",
    "                scheduler.step(epoch_bleu) # decay learning rate if necessary\n",
    "                if epoch_bleu > best_dev_bleu or (\n",
    "                        epoch_bleu == best_dev_bleu and\\\n",
    "                        last_train_bleu > best_train_bleu): # improvement detected!\n",
    "                    # update best dev and train bleu\n",
    "                    best_dev_bleu = epoch_bleu\n",
    "                    best_train_bleu = last_train_bleu\n",
    "                    # update best weights\n",
    "                    best_wts = copy.deepcopy(nmt.state_dict())\n",
    "                    print('\\t*** improvement detected! best_dev_bleu=%f' % best_dev_bleu)\n",
    "            else:\n",
    "                last_train_bleu = epoch_bleu\n",
    "\n",
    "    print()\n",
    "    elapsed_time = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        elapsed_time // 60, elapsed_time % 60))\n",
    "    print('Best dev bleu: {:4f}'.format(best_dev_bleu))\n",
    "    \n",
    "    return dict(\n",
    "        best_wts=best_wts,\n",
    "        history=history,\n",
    "        best_dev_bleu=best_dev_bleu,\n",
    "        best_train_bleu=best_train_bleu,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuántos parámetros entrenables tiene el modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40884052"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trainable_parameters(nmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo tiene 40884052 parámetros entrenables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿De qué tamaño es el vocabulario del lenguaje de origen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50004"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.src_id2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un vocabulario de 50004 palabras, 50000 si no tomamos en cuenta los 4 tokens especiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿De qué tamaño es el vocabulario del lenguaje de destino?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50004"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.dst_id2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un vocabulario de 50004 palabras, 50000 si no tomamos en cuenta los 4 tokens especiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo afecta el tamaño de ambos lenguajes a la complejidad del problema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del lenguaje de origen, un vocabulario más grande implica más word embeddings que aprender.\n",
    "\n",
    "En el caso del lenguaje de destino, un vocabulario más grande implica dos cosas:\n",
    "- más word embeddings que aprender\n",
    "- una softmax más grande (y por ende costosa) para predecir el siguiente token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, factor=LR_DECAY, patience=DECAY_PATIENCE, mode='max', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resuming training from checkpoint = ./nmt.pth ....\n",
      "\n",
      "====== [Epoch 5/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.87269, running_bleu=0.23288, elapsed_time=13m 48s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=3.99866, running_bleu=0.19847, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.198474\n",
      "\n",
      "====== [Epoch 6/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.83863, running_bleu=0.24167, elapsed_time=13m 48s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=3.94992, running_bleu=0.19993, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.199925\n",
      "\n",
      "====== [Epoch 7/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.81294, running_bleu=0.24874, elapsed_time=13m 47s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.00441, running_bleu=0.19995, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.199945\n",
      "\n",
      "====== [Epoch 8/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.79182, running_bleu=0.25477, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.06962, running_bleu=0.20246, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.202455\n",
      "\n",
      "====== [Epoch 9/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.77469, running_bleu=0.26004, elapsed_time=13m 48s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.09278, running_bleu=0.20692, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.206917\n",
      "\n",
      "====== [Epoch 10/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.75910, running_bleu=0.26533, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.07170, running_bleu=0.20589, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 11/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.74745, running_bleu=0.26889, elapsed_time=13m 48s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.04227, running_bleu=0.20758, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.207581\n",
      "\n",
      "====== [Epoch 12/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.73513, running_bleu=0.27286, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.07922, running_bleu=0.21373, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.213727\n",
      "\n",
      "====== [Epoch 13/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.72671, running_bleu=0.27651, elapsed_time=13m 48s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.19156, running_bleu=0.20435, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 14/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.71869, running_bleu=0.27949, elapsed_time=13m 48s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.19155, running_bleu=0.21228, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 15/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.71350, running_bleu=0.28096, elapsed_time=13m 48s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.16581, running_bleu=0.20904, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 16/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.70606, running_bleu=0.28391, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.17856, running_bleu=0.20575, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 17/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.70104, running_bleu=0.28564, elapsed_time=13m 50s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.07261, running_bleu=0.20875, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 18/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.69604, running_bleu=0.28814, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.33307, running_bleu=0.20672, elapsed_time=0m 2s\n",
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-04.\n",
      "\n",
      "====== [Epoch 19/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.62900, running_bleu=0.31865, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.17816, running_bleu=0.21732, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.217319\n",
      "\n",
      "====== [Epoch 20/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.59280, running_bleu=0.33562, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.16673, running_bleu=0.21787, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.217873\n",
      "\n",
      "====== [Epoch 21/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.57452, running_bleu=0.34481, elapsed_time=13m 48s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.20180, running_bleu=0.21732, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 22/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.55895, running_bleu=0.35213, elapsed_time=13m 50s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.27847, running_bleu=0.21404, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 23/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.54686, running_bleu=0.35821, elapsed_time=13m 51s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.24102, running_bleu=0.21558, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 24/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.53707, running_bleu=0.36456, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.33736, running_bleu=0.21547, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 25/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.52761, running_bleu=0.37013, elapsed_time=13m 50s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.30704, running_bleu=0.21920, elapsed_time=0m 2s\n",
      "\t*** improvement detected! best_dev_bleu=0.219195\n",
      "\n",
      "====== [Epoch 26/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.51950, running_bleu=0.37435, elapsed_time=13m 50s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.28254, running_bleu=0.21777, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 27/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.51178, running_bleu=0.37841, elapsed_time=13m 50s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.30072, running_bleu=0.21305, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 28/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 1701/6770, running_loss=0.48422, running_bleu=0.39760, elapsed_time=3m 28s\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 6770/6770, running_loss=0.35811, running_bleu=0.49848, elapsed_time=13m 49s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.63762, running_bleu=0.21094, elapsed_time=0m 2s\n",
      "\n",
      "====== [Epoch 49/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.35655, running_bleu=0.50006, elapsed_time=13m 50s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.66411, running_bleu=0.20963, elapsed_time=0m 2s\n",
      "Epoch    49: reducing learning rate of group 0 to 3.1250e-05.\n",
      "\n",
      "====== [Epoch 50/50]\n",
      "--------------------- train ---------------------\n",
      "Batch: 6770/6770, running_loss=0.35169, running_bleu=0.50460, elapsed_time=13m 50s\n",
      "--------------------- dev ---------------------\n",
      "Batch: 27/27, running_loss=4.67841, running_bleu=0.20839, elapsed_time=0m 2s\n",
      "\n",
      "Training complete in 637m 20s\n",
      "Best dev bleu: 0.219195\n"
     ]
    }
   ],
   "source": [
    "ret = train_model(nmt, optimizer, criterion, scheduler, dataloaders, epochs=EPOCHS, checkpoint_path='./nmt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ret, './nmt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_dev_curves(history, figsize=(12, 6)):\n",
    "    epochs = list(range(1, len(history['train_loss'])+1))\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    eps = 0.9\n",
    "    \n",
    "    ax = plt.subplot(121)\n",
    "    ax.set_xlim(epochs[0]-eps, epochs[-1]+eps)\n",
    "    ax.set_title('Loss per epoch')\n",
    "    ax.plot(epochs, history['train_loss'], label='train_loss')\n",
    "    ax.plot(epochs, history['dev_loss'], label='dev_loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')    \n",
    "    ax.legend()\n",
    "    \n",
    "    ax = plt.subplot(122)\n",
    "    ax.set_xlim(epochs[0]-eps, epochs[-1]+eps)\n",
    "    ax.set_title('Bleu per epoch')\n",
    "    ax.plot(epochs, history['train_bleu'], label='train_bleu')\n",
    "    ax.plot(epochs, history['dev_bleu'], label='dev_bleu')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Bleu')    \n",
    "    best_dev_bleu, best_dev_i = max((a,i) for i,a in enumerate(history['dev_bleu']))\n",
    "    ax.hlines(best_dev_bleu, epochs[0], epochs[-1], colors=('red',), linestyles='dashed',\n",
    "               label='best_dev_bleu=%.3f, epoch=%d' % (best_dev_bleu, best_dev_i))\n",
    "    best_train_bleu, best_train_i = max((a,i) for i,a in enumerate(history['train_bleu']))\n",
    "    ax.hlines(best_train_bleu, epochs[0], epochs[-1], colors=('green',), linestyles='dashed',\n",
    "               label='best_train_bleu=%.3f, epoch=%d' % (best_train_bleu, best_train_i))    \n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./nmt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_dev_curves(checkpoint['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arriba tenemos las curvas de loss por epoch y bleu por epoch.\n",
    "Hay un patrón algo extraño que aún no entiendo bien por qué pasa. Por un lado la dev_loss empeora con el tiempo, sin embargo el dev_bleu sí mejora (al menos por un tiempo hasta que se queda estancado en torno al 0.2, con un peak de 0.219 en la época 25). Por su parte, el train_loss siempre baja y el train_blue siempre sube. Esto parece indicar un sobreajuste, en el sentido que el modelo sigue aprendiendo del train pero esto no se ve reflejado en mejoras en el set de dev. Sin embargo, no podemos descartar de plano que, si dejamos el modelo entrenando harto rato más, quizás se encuentre una nueva solución en el espacio de hipótesis mucho mejor y que la generalización al dev set mejore harto más. Pero si tuviese que apostar, diría que probablemente no va a mejorar mucho más en su generalización al set de dev."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACLARACIÓN: dado que por alguna extraña razón la loss crecía en vez de bajar en validación, me vi obligado a usar el BLEU score como criterio para escoger los mejores pesos, que igual tiene mucho sentido ya que esa es la métrica que al fin y al cabo queremos optimizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_cleaned_sentences_and_bleu_score(\n",
    "    batch_src_sentences,\n",
    "    batch_dst_gt_sentences,\n",
    "    batch_dst_pred_sentences,\n",
    "    smoothing_function,    \n",
    "    src_sentences_list,\n",
    "    dst_gt_sentences_list,\n",
    "    dst_pred_sentences_list,\n",
    "    bleu_scores_list,\n",
    "):    \n",
    "    batch_size = batch_src_sentences.shape[0]\n",
    "    batch_blue = 0\n",
    "    for i in range(batch_size):\n",
    "        clean_src = [t for t in batch_src_sentences[i] if t >= 4]\n",
    "        clean_dst_gt = [t for t in batch_dst_gt_sentences[i] if t >= 4]\n",
    "        clean_dst_pred = [t for t in batch_dst_pred_sentences[i] if t >= 4]\n",
    "        bleu = sentence_bleu((clean_dst_gt,), clean_dst_pred, smoothing_function=smoothing_function)\n",
    "        src_sentences_list.append(clean_src)\n",
    "        dst_gt_sentences_list.append(clean_dst_gt)\n",
    "        dst_pred_sentences_list.append(clean_dst_pred)\n",
    "        bleu_scores_list.append(bleu)\n",
    "        batch_blue += bleu\n",
    "    return batch_blue / batch_size\n",
    "\n",
    "def test_model(nmt, checkpoint, criterion, dataloader):    \n",
    "    \n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    best_wts = checkpoint['best_wts']\n",
    "    nmt.load_state_dict(best_wts)\n",
    "    nmt.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_bleu = 0\n",
    "    running_count = 0\n",
    "    since = time.time()\n",
    "    \n",
    "    src_sentences_list = []\n",
    "    dst_gt_sentences_list = []\n",
    "    dst_pred_sentences_list = []\n",
    "    bleu_scores_list = []\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        \n",
    "        for batch_idx, batch_dict in enumerate(dataloader):\n",
    "            \n",
    "            src_sentences = batch_dict['src'].to(DEVICE)\n",
    "            src_lengths = batch_dict['src_lengths'].to(DEVICE)\n",
    "            dst_sentences = batch_dict['dst'].to(DEVICE)\n",
    "            dst_lengths = batch_dict['dst_lengths'].to(DEVICE)\n",
    "            batch_size = src_sentences.size(0)\n",
    "            output = nmt.eval_forward(\n",
    "                src_sentences,\n",
    "                src_lengths,\n",
    "                dst_lengths)\n",
    "\n",
    "            batch_loss = criterion(output.view(-1,output.shape[-1]),\n",
    "                             dst_sentences.view(-1))\n",
    "            batch_bleu = append_cleaned_sentences_and_bleu_score(src_sentences.cpu().numpy(),\n",
    "                                                                 dst_sentences.cpu().numpy(),\n",
    "                                                                 output.argmax(-1).cpu().numpy(),\n",
    "                                                                 smoothing_function,\n",
    "                                                                 src_sentences_list,\n",
    "                                                                 dst_gt_sentences_list,\n",
    "                                                                 dst_pred_sentences_list,\n",
    "                                                                 bleu_scores_list)\n",
    "            # statistics\n",
    "            running_loss += batch_loss.item() * batch_size\n",
    "            running_bleu += batch_bleu * batch_size\n",
    "            running_count += batch_size\n",
    "\n",
    "            if (batch_idx % 10 == 0 or batch_idx + 1 == len(dataloader)):\n",
    "                elapsed_time = time.time() - since\n",
    "                print(\"Batch: %d/%d, running_loss=%.5f, running_bleu=%.5f, elapsed_time=%.0fm %.0fs\" % (\n",
    "                    batch_idx+1, len(dataloader),\n",
    "                    running_loss/running_count,\n",
    "                    running_bleu/running_count,\n",
    "                    elapsed_time // 60, elapsed_time % 60,\n",
    "                ), end=\"\\r\", flush=True)\n",
    "\n",
    "    print()\n",
    "    avg_loss = running_loss / running_count\n",
    "    avg_bleu = running_bleu / running_count\n",
    "    assert abs(avg_bleu - sum(bleu_scores_list) / len(bleu_scores_list)) < 1e-8\n",
    "    elapsed_time = time.time() - since\n",
    "    print('Test complete in {:.0f}m {:.0f}s'.format(\n",
    "        elapsed_time // 60, elapsed_time % 60))\n",
    "    print('Test loss: {:4f}\\t Test bleu: {:4f}'.format(avg_loss, avg_bleu))    \n",
    "    \n",
    "    return dict(\n",
    "        src_sentences_list = src_sentences_list,\n",
    "        dst_gt_sentences_list = dst_gt_sentences_list,\n",
    "        dst_pred_sentences_list = dst_pred_sentences_list,\n",
    "        bleu_scores_list = bleu_scores_list,\n",
    "    )\n",
    "\n",
    "def display_examples(src_sentences_list, dst_gt_sentences_list, dst_pred_sentences_list, bleu_scores_list,\n",
    "                     ids2tokens_func, src_id2token, dst_id2token,\n",
    "                     top_k=5, worst_k=5, random_k=5, min_src_length=0):\n",
    "    \n",
    "    indexes = [i for i in range(len(bleu_scores_list)) if len(src_sentences_list[i]) >= min_src_length]\n",
    "    indexes.sort(key=lambda i : bleu_scores_list[i], reverse=True)\n",
    "    n = len(indexes)\n",
    "    \n",
    "    if top_k > 0:        \n",
    "        print('=========== TOP %d translations ===============\\n' % (top_k))        \n",
    "        for i in range(top_k):\n",
    "            j = indexes[i]\n",
    "            src_sentence = ids2tokens_func(src_id2token, src_sentences_list[j])\n",
    "            dst_gt_sentence = ids2tokens_func(dst_id2token, dst_gt_sentences_list[j])\n",
    "            dst_pred_sentence = ids2tokens_func(dst_id2token, dst_pred_sentences_list[j])\n",
    "            bleu = bleu_scores_list[j]\n",
    "            print('******** top %d (bleu_score = %.4f) *******' % (i+1, bleu))\n",
    "            print('--- SOURCE:\\n', src_sentence)\n",
    "            print('--- MODEL:\\n', dst_pred_sentence)\n",
    "            print('--- GT:\\n', dst_gt_sentence)\n",
    "            print()\n",
    "    \n",
    "    if worst_k > 0:        \n",
    "        print('=========== WORST %d translations ===============\\n' % (worst_k))        \n",
    "        for i in range(1, worst_k+1):\n",
    "            j = indexes[-i]\n",
    "            src_sentence = ids2tokens_func(src_id2token, src_sentences_list[j])\n",
    "            dst_gt_sentence = ids2tokens_func(dst_id2token, dst_gt_sentences_list[j])\n",
    "            dst_pred_sentence = ids2tokens_func(dst_id2token, dst_pred_sentences_list[j])\n",
    "            bleu = bleu_scores_list[j]\n",
    "            print('******** worst %d (bleu_score = %.4f) *******' % (i, bleu))\n",
    "            print('--- SOURCE:\\n', src_sentence)\n",
    "            print('--- MODEL:\\n', dst_pred_sentence)\n",
    "            print('--- GT:\\n', dst_gt_sentence)\n",
    "            print()\n",
    "\n",
    "    if random_k > 0:        \n",
    "        print('=========== Random %d translations ===============\\n' % (random_k))\n",
    "        random_idxs = set()\n",
    "        while len(random_idxs) < random_k:\n",
    "            i = random.randint(0, n-1)\n",
    "            if i in random_idxs:\n",
    "                continue\n",
    "            if i < top_k or i >= n - worst_k:\n",
    "                continue\n",
    "            random_idxs.add(i)\n",
    "\n",
    "        for i in random_idxs:\n",
    "            j = indexes[i]\n",
    "            src_sentence = ids2tokens_func(src_id2token, src_sentences_list[j])\n",
    "            dst_gt_sentence = ids2tokens_func(dst_id2token, dst_gt_sentences_list[j])\n",
    "            dst_pred_sentence = ids2tokens_func(dst_id2token, dst_pred_sentences_list[j])\n",
    "            bleu = bleu_scores_list[j]\n",
    "            print('******** random %d (bleu_score = %.4f) *******' % (i+1, bleu))\n",
    "            print('--- SOURCE:\\n', src_sentence)\n",
    "            print('--- MODEL:\\n', dst_pred_sentence)\n",
    "            print('--- GT:\\n', dst_gt_sentence)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados en test y comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 252/252, running_loss=4.08037, running_bleu=0.20081, elapsed_time=0m 18s\n",
      "Test complete in 0m 18s\n",
      "Test loss: 4.080375\t Test bleu: 0.200807\n"
     ]
    }
   ],
   "source": [
    "test_result = test_model(nmt, checkpoint, criterion, dataloaders['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, se obtuvo una test loss de 4.08 y un test bleu de 0.2, muy en el orden de los valores obtenidos en el set de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traducciones de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== TOP 2 translations ===============\n",
      "\n",
      "******** top 1 (bleu_score = 0.8253) *******\n",
      "--- SOURCE:\n",
      " But at the same time, it's been increasingly frustrating  because we have only started to measure the damage that we have done.\n",
      "--- MODEL:\n",
      " Pero al mismo tiempo, ha sido cada vez ms frustrante porque solo hemos comenzado a medir el dao que hemos hecho.\n",
      "--- GT:\n",
      " Pero al mismo tiempo, ha sido cada vez ms frustrante porque solo hemos estado empezando a medir el dao que hemos hecho.\n",
      "\n",
      "******** top 2 (bleu_score = 0.7771) *******\n",
      "--- SOURCE:\n",
      " They wanted nothing to do with this film, mainly because they would have no control,  they would have no control over the final product.\n",
      "--- MODEL:\n",
      " Ellos queran que no queran nada que ver con esta pelcula, principalmente porque no tendran control, no tendran control sobre el producto final.\n",
      "--- GT:\n",
      " No queran tener nada que ver con esta pelcula, principalmente porque no tendran control, no tendran control sobre el producto final.\n",
      "\n",
      "=========== WORST 3 translations ===============\n",
      "\n",
      "******** worst 1 (bleu_score = 0.0062) *******\n",
      "--- SOURCE:\n",
      " So of course, random sequences of instructions  are very unlikely to sort numbers,  so none of them will really do it.\n",
      "--- MODEL:\n",
      " Por supuesto, por supuesto, secuencias de instrucciones de instrucciones muy poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco poco\n",
      "--- GT:\n",
      " Claro, es poco probable que unas secuencias aleatorias de instrucciones, nmeros, as que ninguna de ellas lo logr.\n",
      "\n",
      "******** worst 2 (bleu_score = 0.0086) *******\n",
      "--- SOURCE:\n",
      " And so --    It says, \"This box belongs to ... \"  And so I type in my name.\n",
      "--- MODEL:\n",
      " Y                         \n",
      "--- GT:\n",
      " Y as...  Dice: \"Esta caja pertenece Escribo mi nombre.\n",
      "\n",
      "******** worst 3 (bleu_score = 0.0098) *******\n",
      "--- SOURCE:\n",
      " When the Golden Cat Corporation  got rid of their 10 cat litter products,  they saw an increase in profits  by 87 percent --  a function of both increase in sales  and lowering of costs.\n",
      "--- MODEL:\n",
      " Cuando el de la regla de se de sus 10  un incremento en ganancias  por ciento del 87 % --  una funcin del aumento de ambos de los\n",
      "--- GT:\n",
      " Cuando la corporacin elimin los productos para de gatos que menos se se aumentaron las en por dos mayores ventas y menores costos.\n",
      "\n",
      "=========== Random 5 translations ===============\n",
      "\n",
      "******** random 297 (bleu_score = 0.3738) *******\n",
      "--- SOURCE:\n",
      " I can't reveal many details about how I left North Korea,  but I only can say that during the ugly years of the famine,  I was sent to China to live with distant relatives.\n",
      "--- MODEL:\n",
      " No puedo revelar muchos detalles sobre cmo me dej Corea del Norte, pero slo puedo decir que durante los aos del  me enviaron a China a vivir con los parientes\n",
      "--- GT:\n",
      " no puedo revelar los detalles de cmo sal de Corea del Norte, pero solo puedo decir que durante los aos de la hambruna fui enviada a China a vivir con parientes\n",
      "\n",
      "******** random 1475 (bleu_score = 0.1508) *******\n",
      "--- SOURCE:\n",
      " Henry, by virtue of being assumed to be male --  although I haven't told you that he's the XY one --  by virtue of being assumed to be male  is now liable to be  which Mary does not need to worry about.\n",
      "--- MODEL:\n",
      " Henry, por virtud de ser hombre --  aunque no les he dicho que es la  por virtud de ser  por la que ser  es la  Que Mary no necesita\n",
      "--- GT:\n",
      " Henry, en virtud de ser considerado hombre -si bien no les he dicho si es en virtud de ser considerado hombre ahora es del obligatorio del que Mary no tiene que preocuparse.\n",
      "\n",
      "******** random 2212 (bleu_score = 0.0507) *******\n",
      "--- SOURCE:\n",
      " The great 20th-century geneticist,   who was also a  in the Russian Orthodox Church,  once wrote an essay that he titled  \"Nothing in Biology Makes  Except in the Light of\n",
      "--- MODEL:\n",
      " Los grandes   que tambin era un en la  una vez escribi un ensayo que  en  en la\n",
      "--- GT:\n",
      " El gran genetista del siglo XX, que adems en la Iglesia escribi un ensayo titulado \"Nada en biologa tiene sentido salvo a la luz de la\n",
      "\n",
      "******** random 1045 (bleu_score = 0.2122) *******\n",
      "--- SOURCE:\n",
      " Today I want to talk  about one of the biggest modern day choosing problems that we have,  which is the choice overload problem.\n",
      "--- MODEL:\n",
      " Hoy quiero hablar  de uno de los mayores das modernos que eligen problemas que tenemos,  que es el problema de la eleccin del\n",
      "--- GT:\n",
      " Hoy quiero hablar de uno de los mayores problemas de la elegir ante la sobrecarga de opciones.\n",
      "\n",
      "******** random 1616 (bleu_score = 0.1283) *******\n",
      "--- SOURCE:\n",
      " So my first impulse in the face of all of this attention  was to become very protective of my own relationship.\n",
      "--- MODEL:\n",
      " As que mi primer impulso en la cara de toda esta atencin era ser muy de mi propia relacin.\n",
      "--- GT:\n",
      " As que, mi primer impulso ante toda esta atencin fue ser muy reservada con mi propia relacin.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_examples(\n",
    "    test_result['src_sentences_list'],\n",
    "    test_result['dst_gt_sentences_list'],\n",
    "    test_result['dst_pred_sentences_list'],\n",
    "    test_result['bleu_scores_list'],\n",
    "    test_dataset.ids2tokens,\n",
    "    test_dataset.src_id2token,\n",
    "    test_dataset.dst_id2token,\n",
    "    top_k = 2,\n",
    "    worst_k=3,\n",
    "    random_k = 5,\n",
    "    min_src_length = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arriba hay 5 traducciones de ejemplo (las 2 mejores y las 3 peores), todas con un src_length >= 20. Las escogí porque quería ver los mejores casos y los peores casos con un largo desafiante (20 o más). También agregué 5 ejemplos adicionales muestreados aleatoriamente (también con src_lenth >= 20) por un tema de curiosidad, pero el lector puede limitarse a los primeros 5 ejemplos si así lo desea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 7\n",
    "### Limitaciones del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Falta usar máscaras para ignorar el padding**: Cuando el **decoder** calcula los coeficientes de atención sobre los hidden states del **encoder**, falta aplicar una máscara para ignorar los hidden states del **encoder** asociados a los tokens de padding. Explicación: el encoder retorna un tensor con todos los hidden states de las oraciones del lenguaje de origen, ajustado al tamaño de la oración más larga, y los hidden states de las oraciones más cortas son rellenados por la derecha con vectores de puros 0s. Al momento de calcular la softmax, estos hidden states de relleno no son ignorados (tienen un peso no normalizado de $e^0 = 1$ que no es 0) lo cual puede meter ruido. Sería más correcto aplicar una máscara para ignorar el padding y forzar que la atención se distribuya sobre la parte relevante de la oración de origen.\n",
    "\n",
    "2. **Se pierde vocabulario**: al usar un threshold de frecuencia, uno está forzado a ignorar palabras y por ende perder vocabulario. Por el otro lado, si permitimos que se usen todas las palabras, el vocabulario resultante puede ser muy grande y habría que aprender demasiados word embeddings. Una forma de resolver ambos problemas es deshacerse de los word-embeddings por completo y trabajar a un nivel más atómico. Ejemplos de esto son trabajar a nivel de caracteres (_character-level_ embeddings) o un nivel intermedio (_word-piece_ embeddings, Byte-Pair Encoding).\n",
    "\n",
    "3. **Atrasado respecto al estado del arte**: Los modelos estado de arte actuales en NLP están basados de una u otra manera en el Transformer (del paper Attention is All You Need). La ventaja del transformer es que puede procesar secuencias completas en paralelo y encontrar relaciones entre elementos de una secuencia en O(1) pasos, mientras que modelos recurrentes como la LSTM no son paralelizables (hay que iterar por cada elemento de la secuencia un paso a la vez) y además es más difícil relacionar elementos distantes de una secuencia (debido a los muchos pasos intermedios antes de que la información de un extremo llegue al otro extremo en una secuencia)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
